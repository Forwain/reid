{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('datasets/prcc_proprecess')\n",
    "os.makedirs('datasets/prcc_proprecess/train')\n",
    "os.makedirs('datasets/prcc_proprecess/query')\n",
    "os.makedirs('datasets/prcc_proprecess/gallery')\n",
    "os.makedirs('datasets/prcc_proprecess/train_sketch')\n",
    "os.makedirs('datasets/prcc_proprecess/query_sketch')\n",
    "os.makedirs('datasets/prcc_proprecess/gallery_sketch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.vision.all import get_image_files\n",
    "import shutil\n",
    "train_set=set()\n",
    "\n",
    "for root, dirs, files in os.walk(\"/kaggle/input/prccdata1/prcc/rgb/train\"):\n",
    "  for dir in dirs:\n",
    "    train_set.add(dir)\n",
    "    path=os.path.join(root,dir)\n",
    "    image_names=get_image_files(path)\n",
    "    for i,name in enumerate(image_names):\n",
    "       name=str(name)\n",
    "       last_name=name.split('/')[-1]\n",
    "       cam=last_name[0]\n",
    "       if cam=='A':\n",
    "            cam=0\n",
    "       elif cam=='B':\n",
    "            cam=1\n",
    "       elif cam=='C':\n",
    "            cam=2\n",
    "       \n",
    "       last_name=last_name.split('.')[0][-3:]\n",
    "       new_name=os.path.join('datasets','prcc_proprecess','train',dir+'_'+str(cam)+'_'+last_name+'.jpg')\n",
    "       shutil.copy(str(name),new_name)\n",
    "  break\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for root, dirs, files in os.walk(\"/kaggle/input/prccdata1/prcc/sketch/train\"):\n",
    "  for dir in dirs:\n",
    "    train_set.add(dir)\n",
    "    path=os.path.join(root,dir)\n",
    "    image_names=get_image_files(path)\n",
    "\n",
    "    for i,name in enumerate(image_names):\n",
    "       name=str(name)\n",
    "       last_name=name.split('/')[-1]\n",
    "       cam=last_name[0]\n",
    "       if cam=='A':\n",
    "            cam=0\n",
    "       elif cam=='B':\n",
    "            cam=1\n",
    "       elif cam=='C':\n",
    "            cam=2\n",
    "       last_name=last_name.split('.')[0][-3:]\n",
    "       new_name=os.path.join('datasets','prcc_proprecess','train_sketch',dir+'_'+str(cam)+'_'+last_name+'.jpg')\n",
    "       shutil.copy(str(name),new_name)\n",
    "  break\n",
    "\n",
    "\n",
    "for root, dirs, files in os.walk(\"/kaggle/input/prccdata1/prcc/rgb/val\"):\n",
    "  for dir in dirs:\n",
    "    train_set.add(dir)\n",
    "    path=os.path.join(root,dir)\n",
    "    image_names=get_image_files(path)\n",
    "    A_query=True\n",
    "    B_query=True\n",
    "    C_query=True\n",
    "    for i,name in enumerate(image_names):\n",
    "      name=str(name)\n",
    "      last_name=name.split('/')[-1]\n",
    "      \n",
    "      if last_name[0]=='A': \n",
    "           if A_query:\n",
    "\n",
    "               last_name=last_name.split('.')[0][-3:]\n",
    "               new_name=os.path.join('datasets','prcc_proprecess','query',dir+'_0'+'_'+last_name+'.jpg')\n",
    "\n",
    "               A_query=False\n",
    "           else:\n",
    "               last_name=last_name.split('.')[0][-3:]\n",
    "               new_name=os.path.join('datasets','prcc_proprecess','gallery',dir+'_0'+'_'+last_name+'.jpg')\n",
    "      \n",
    "      elif last_name[0]=='B': \n",
    "           if B_query:\n",
    "\n",
    "               last_name=last_name.split('.')[0][-3:]\n",
    "               new_name=os.path.join('datasets','prcc_proprecess','query',dir+'_1'+'_'+last_name+'.jpg')\n",
    "               B_query=False\n",
    "           else:\n",
    "               last_name=last_name.split('.')[0][-3:]\n",
    "               new_name=os.path.join('datasets','prcc_proprecess','gallery',dir+'_1'+'_'+last_name+'.jpg')\n",
    "     \n",
    "      elif last_name[0]=='C' :\n",
    "           if C_query:\n",
    "\n",
    "               last_name=last_name.split('.')[0][-3:]\n",
    "               new_name=os.path.join('datasets','prcc_proprecess','query',dir+'_2'+'_'+last_name+'.jpg')\n",
    "               C_query=False\n",
    "           else:\n",
    "               last_name=last_name.split('.')[0][-3:]\n",
    "               new_name=os.path.join('datasets','prcc_proprecess','gallery',dir+'_2'+'_'+last_name+'.jpg')\n",
    "      shutil.copy(name,new_name)\n",
    "      \n",
    "              \n",
    "  break\n",
    "\n",
    "for root, dirs, files in os.walk(\"/kaggle/input/prccdata1/prcc/sketch/val\"):\n",
    "  for dir in dirs:\n",
    "    train_set.add(dir)\n",
    "    path=os.path.join(root,dir)\n",
    "    image_names=get_image_files(path)\n",
    "    A_query=True\n",
    "    B_query=True\n",
    "    C_query=True\n",
    "    for i,name in enumerate(image_names):\n",
    "      name=str(name)\n",
    "      last_name=name.split('/')[-1]\n",
    "      \n",
    "      if last_name[0]=='A':\n",
    "           if A_query:\n",
    "\n",
    "               last_name=last_name.split('.')[0][-3:]\n",
    "               new_name=os.path.join('datasets','prcc_proprecess','query_sketch',dir+'_0'+'_'+last_name+'.jpg')\n",
    "               A_query=False\n",
    "           else:\n",
    "               last_name=last_name.split('.')[0][-3:]\n",
    "               new_name=os.path.join('datasets','prcc_proprecess','gallery_sketch',dir+'_0'+'_'+last_name+'.jpg')\n",
    "      \n",
    "      elif last_name[0]=='B': \n",
    "           if B_query:\n",
    "\n",
    "               last_name=last_name.split('.')[0][-3:]\n",
    "               new_name=os.path.join('datasets','prcc_proprecess','query_sketch',dir+'_1'+'_'+last_name+'.jpg')\n",
    "               B_query=False\n",
    "           else:\n",
    "               last_name=last_name.split('.')[0][-3:]\n",
    "               new_name=os.path.join('datasets','prcc_proprecess','gallery_sketch',dir+'_1'+'_'+last_name+'.jpg')\n",
    "\n",
    "      elif last_name[0]=='C': \n",
    "           if C_query:\n",
    "\n",
    "               last_name=last_name.split('.')[0][-3:]\n",
    "               new_name=os.path.join('datasets','prcc_proprecess','query_sketch',dir+'_2'+'_'+last_name+'.jpg')\n",
    "               C_query=False\n",
    "           else:\n",
    "               last_name=last_name.split('.')[0][-3:]\n",
    "               new_name=os.path.join('datasets','prcc_proprecess','gallery_sketch',dir+'_2'+'_'+last_name+'.jpg')\n",
    "      shutil.copy(name,new_name)\n",
    "      \n",
    "  break\n",
    "print(train_set)\n",
    "train_set=list(train_set)\n",
    "print(train_set)\n",
    "train_label={name:i for i,name in enumerate(train_set)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as T\n",
    "def build_change_transforms(cfg, training=True):\n",
    "    normalize_transform = T.Normalize(mean=cfg['mean'], std=cfg['std'])\n",
    "    if training:\n",
    "        transform = T.Compose([\n",
    "            T.Resize(cfg['train_size']),\n",
    "            T.ColorJitter(),\n",
    "            T.RandomGrayscale(p=0.05),\n",
    "            T.ToTensor(),\n",
    "            normalize_transform,\n",
    "        ])\n",
    "    else:\n",
    "        transform = T.Compose([\n",
    "            T.Resize(cfg['test_size']),\n",
    "            T.ToTensor(),\n",
    "            normalize_transform\n",
    "        ])\n",
    "\n",
    "    return transform\n",
    "\n",
    "\n",
    "import albumentations as A\n",
    "def build_transform(cfg,training=True):\n",
    "  if training:\n",
    "    transform=A.Compose([\n",
    "        A.Resize(228,228),\n",
    "        A.RandomBrightnessContrast(),\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.Normalize()\n",
    "    ])\n",
    "  else:\n",
    "    transform=A.Compose([\n",
    "        A.Resize(228,228),\n",
    "        A.Normalize()\n",
    "    ])\n",
    "  return transform\n",
    "from fastai.vision.all import get_image_files\n",
    "import torch.utils.data as Data\n",
    "import json\n",
    "import os.path as osp\n",
    "\n",
    "class PRCC:\n",
    "  def __init__(self,root):\n",
    "    self.root=root\n",
    "    self.train_dir=osp.join(self.root,'train')\n",
    "    self.query_dir=osp.join(self.root,'query')\n",
    "    self.gallery_dir=osp.join(self.root,'gallery')\n",
    "    self.train_sketch_dir=osp.join(self.root,'train_sketch')\n",
    "    self.query_sketch_dir=osp.join(self.root,'query_sketch')\n",
    "    self.gallery_sketch_dir=osp.join(self.root,'gallery_sketch')\n",
    "    \n",
    "    \n",
    "    \n",
    "    train=self._process_dir(self.train_dir)\n",
    "    query=self._process_dir(self.query_dir)\n",
    "    gallery=self._process_dir(self.gallery_dir)\n",
    "    train_sketch=self._process_dir(self.train_sketch_dir)\n",
    "    query_sketch=self._process_dir(self.query_sketch_dir)\n",
    "    gallery_sketch=self._process_dir(self.gallery_sketch_dir)\n",
    "\n",
    "\n",
    "    self.train=train\n",
    "    self.query=query\n",
    "    self.gallery=gallery\n",
    "    self.train_sketch=train_sketch\n",
    "    self.query_sketch=query_sketch\n",
    "    self.gallery_sketch=gallery_sketch\n",
    "    self.num_train_pids, self.num_train_imgs, self.num_train_cams = self.get_imagedata_info(self.train)\n",
    "    self.num_query_pids, self.num_query_imgs, self.num_query_cams = self.get_imagedata_info(self.query)\n",
    "    self.num_gallery_pids, self.num_gallery_imgs, self.num_gallery_cams = self.get_imagedata_info(self.gallery)\n",
    "\n",
    "  \n",
    "  def get_imagedata_info(self,data):\n",
    "    pids,cams=[],[]\n",
    "    for _,pid,camid in data:\n",
    "      pids+=[pid]\n",
    "      cams+=[camid]\n",
    "    pids=set(pids)\n",
    "    cams=set(cams)\n",
    "    num_pids=len(pids)\n",
    "    num_cams=len(cams)\n",
    "    num_imgs=len(data)\n",
    "    return num_pids,num_imgs,num_cams\n",
    "\n",
    "  def _process_dir(self,dir_path):\n",
    "    image_names=get_image_files(dir_path)\n",
    "    dataset=[]\n",
    "    pid_set=set()\n",
    "    for name in image_names:\n",
    "      name=str(name)\n",
    "      path=name.split('/')[-1]\n",
    "      pid,cam,_=path.split('_')\n",
    "      pid=train_label[pid]\n",
    "      dataset.append((name,pid,cam))\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import random\n",
    "import torch\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "from torch.utils.data.sampler import Sampler\n",
    "class DateRandomIdentitySampler(Sampler):\n",
    "    \"\"\"\n",
    "    Randomly sample N identities, then for each identity,\n",
    "    randomly sample K instances, therefore batch size is N*K.\n",
    "    Args:\n",
    "    - data_source (list): list of (img_path, pid, camid).\n",
    "    - num_instances (int): number of instances per identity in a batch.\n",
    "    - batch_size (int): number of examples in a batch.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data_source, batch_size, num_instances):\n",
    "        self.data_source = data_source\n",
    "        self.batch_size = batch_size\n",
    "        self.num_instances = num_instances\n",
    "        self.num_pids_per_batch = self.batch_size // self.num_instances\n",
    "        self.index_dic = defaultdict(list)\n",
    "        for index, (_, pid, _) in enumerate(self.data_source):\n",
    "            self.index_dic[pid].append(index)\n",
    "        self.pids = list(self.index_dic.keys())\n",
    "\n",
    "        # estimate number of examples in an epoch\n",
    "        self.length = 0\n",
    "        for pid in self.pids:\n",
    "            idxs = self.index_dic[pid]\n",
    "            num = len(idxs)\n",
    "            if num < self.num_instances:\n",
    "                num = self.num_instances\n",
    "                \n",
    "            self.length += num - num % self.num_instances\n",
    "\n",
    "    def __iter__(self):\n",
    "        batch_idxs_dict = defaultdict(list)\n",
    "\n",
    "        for pid in self.pids:\n",
    "            idxs = copy.deepcopy(self.index_dic[pid])\n",
    "            if len(idxs) < self.num_instances:\n",
    "                idxs = np.random.choice(idxs, size=self.num_instances, replace=True)\n",
    "            random.shuffle(idxs)\n",
    "            batch_idxs = []\n",
    "            for idx in idxs:\n",
    "                batch_idxs.append(idx)\n",
    "                if len(batch_idxs) == self.num_instances:\n",
    "                    batch_idxs_dict[pid].append(batch_idxs)\n",
    "                    batch_idxs = []\n",
    "\n",
    "        avai_pids = copy.deepcopy(self.pids)\n",
    "        final_idxs = []\n",
    "\n",
    "        while len(avai_pids) >= self.num_pids_per_batch:\n",
    "            selected_pids = random.sample(avai_pids, self.num_pids_per_batch)\n",
    "            for pid in selected_pids:\n",
    "                batch_idxs = batch_idxs_dict[pid].pop(0)\n",
    "                final_idxs.extend(batch_idxs)\n",
    "                if len(batch_idxs_dict[pid]) == 0:\n",
    "                    avai_pids.remove(pid)\n",
    "\n",
    "        self.length = len(final_idxs)\n",
    "        return iter(final_idxs)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "import torch.utils.data as Data\n",
    "class ImageDataset(Data.Dataset):\n",
    "    def __init__(self, dataset,data,transform=None):\n",
    "        self.dataset = dataset\n",
    "        self.transform = transform\n",
    "        self.data=data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_path, pid, camid = self.dataset[index]\n",
    "        img = Image.open(img_path)\n",
    "\n",
    "        path=img_path.split('/')[-1]\n",
    "        data_type=img_path.split('/')[-2]\n",
    "        if data_type.startswith('train'):\n",
    "          sketch_dir=self.data.train_sketch_dir\n",
    "        elif data_type.startswith('query'):\n",
    "          sketch_dir=self.data.query_sketch_dir\n",
    "        else:\n",
    "          sketch_dir=self.data.gallery_sketch_dir\n",
    "        sketch_path=os.path.join(sketch_dir,path)\n",
    "        sketch=Image.open(sketch_path)\n",
    "        image=np.array(img)\n",
    "        \n",
    "        sketch=np.array(sketch)\n",
    "       \n",
    "        transform=self.transform(image=image,mask=sketch)\n",
    "        image=transform['image']\n",
    "        sketch=transform['mask']\n",
    "        \n",
    "        image=torch.from_numpy(image.transpose(2,0,1))\n",
    "        sketch=torch.from_numpy(sketch).unsqueeze(0)\n",
    "        sketch=sketch.repeat(3,1,1)\n",
    "        try:\n",
    "          if int(camid)==0 or int(camid)==1:\n",
    "            date=0\n",
    "          else:\n",
    "            date=1\n",
    "        except:\n",
    "          print(camid)\n",
    "        \n",
    "        return image,sketch,int(pid), int(camid),date\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def train_collate_fn1(batch):\n",
    "    imgs, sketch,pids,camid,date = zip(*batch)\n",
    "    pids = torch.tensor(pids, dtype=torch.int64)\n",
    "    return torch.stack(imgs, dim=0),torch.stack(sketch,dim=0), pids\n",
    "\n",
    "###对于验证集而言，为了提高验证的真实性，我们应该防止同一摄像头的图片进入验证（同一摄像头相当于数据泄露）\n",
    "def val_collate_fn1(batch):\n",
    "    imgs,sketch,pids, camids,date = zip(*batch)\n",
    "    return torch.stack(imgs, dim=0),torch.stack(sketch,dim=0),pids, camids,date\n",
    "\n",
    "\n",
    "\n",
    "def make_dataloader(dataset,cfg):\n",
    "    train_transform=build_transform(cfg,training=True)\n",
    "    test_transform=build_transform(cfg,training=False)\n",
    "    \n",
    "   \n",
    "    data=dataset\n",
    "    num_classes=data.num_train_pids\n",
    "    train_set=ImageDataset(data.train,data,train_transform)\n",
    "\n",
    "    train_loader=Data.DataLoader(train_set,batch_size=cfg['train_bs'],\n",
    "                                     sampler=DateRandomIdentitySampler(data.train,\n",
    "                                    cfg['train_bs'],cfg['train_K_instances']),\n",
    "                                    num_workers=cfg['num_workers'],\n",
    "                                    collate_fn=train_collate_fn1)\n",
    "    val_set = ImageDataset(data.query + data.gallery,data,test_transform)\n",
    "    val_loader = Data.DataLoader(\n",
    "        val_set, batch_size=cfg['test_bs'], shuffle=False, num_workers=cfg['num_workers'],\n",
    "        collate_fn=val_collate_fn1\n",
    "    )\n",
    "    return train_loader, val_loader, len(data.query), num_classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "def normalize(x, axis=-1):\n",
    "    \"\"\"Normalizing to unit length along the specified dimension.\n",
    "    Args:\n",
    "      x: pytorch Variable\n",
    "    Returns:\n",
    "      x: pytorch Variable, same shape as input\n",
    "    \"\"\"\n",
    "    x = 1. * x / (torch.norm(x, 2, axis, keepdim=True).expand_as(x) + 1e-12)\n",
    "    return x\n",
    "\n",
    "\n",
    "def euclidean_dist(x, y):\n",
    "    \"\"\" \n",
    "    Args:\n",
    "      x: pytorch Variable, with shape [m, d]\n",
    "      y: pytorch Variable, with shape [n, d]\n",
    "    Returns:\n",
    "      dist: pytorch Variable, with shape [m, n]\n",
    "    \"\"\"\n",
    "    m, n = x.size(0), y.size(0)\n",
    "    xx = torch.pow(x, 2).sum(1, keepdim=True).expand(m, n)\n",
    "    yy = torch.pow(y, 2).sum(1, keepdim=True).expand(n, m).t()\n",
    "    dist = xx + yy\n",
    "    dist.addmm_(1, -2, x, y.t())\n",
    "    dist = dist.clamp(min=1e-12).sqrt()  # for numerical stability\n",
    "    return dist\n",
    "\n",
    "\n",
    "def hard_example_mining(dist_mat, labels, return_inds=False):\n",
    "    \"\"\"For each anchor, find the hardest positive and negative sample.\n",
    "    Args:\n",
    "      dist_mat: pytorch Variable, pair wise distance between samples, shape [N, N]\n",
    "      labels: pytorch LongTensor, with shape [N]\n",
    "      return_inds: whether to return the indices. Save time if `False`(?)\n",
    "    Returns:\n",
    "      dist_ap: pytorch Variable, distance(anchor, positive); shape [N]\n",
    "      dist_an: pytorch Variable, distance(anchor, negative); shape [N]\n",
    "      p_inds: pytorch LongTensor, with shape [N];\n",
    "        indices of selected hard positive samples; 0 <= p_inds[i] <= N - 1\n",
    "      n_inds: pytorch LongTensor, with shape [N];\n",
    "        indices of selected hard negative samples; 0 <= n_inds[i] <= N - 1\n",
    "    NOTE: Only consider the case in which all labels have same num of samples,\n",
    "      thus we can cope with all anchors in parallel.\n",
    "    \"\"\"\n",
    "\n",
    "    assert len(dist_mat.size()) == 2\n",
    "    assert dist_mat.size(0) == dist_mat.size(1)\n",
    "    N = dist_mat.size(0)\n",
    "\n",
    "    # shape [N, N]\n",
    "    is_pos = labels.expand(N, N).eq(labels.expand(N, N).t())\n",
    "    is_neg = labels.expand(N, N).ne(labels.expand(N, N).t())\n",
    "\n",
    "    # `dist_ap` means distance(anchor, positive)\n",
    "    # both `dist_ap` and `relative_p_inds` with shape [N, 1]\n",
    "    dist_ap, relative_p_inds = torch.max(\n",
    "        dist_mat[is_pos].contiguous().view(N, -1), 1, keepdim=True)\n",
    "    # `dist_an` means distance(anchor, negative)\n",
    "    # both `dist_an` and `relative_n_inds` with shape [N, 1]\n",
    "    dist_an, relative_n_inds = torch.min(\n",
    "        dist_mat[is_neg].contiguous().view(N, -1), 1, keepdim=True)\n",
    "    # shape [N]\n",
    "    dist_ap = dist_ap.squeeze(1)\n",
    "    dist_an = dist_an.squeeze(1)\n",
    "\n",
    "    if return_inds:\n",
    "        # shape [N, N]\n",
    "        ind = (labels.new().resize_as_(labels)\n",
    "               .copy_(torch.arange(0, N).long())\n",
    "               .unsqueeze(0).expand(N, N))\n",
    "        # shape [N, 1]\n",
    "        p_inds = torch.gather(\n",
    "            ind[is_pos].contiguous().view(N, -1), 1, relative_p_inds.data)\n",
    "        n_inds = torch.gather(\n",
    "            ind[is_neg].contiguous().view(N, -1), 1, relative_n_inds.data)\n",
    "        # shape [N]\n",
    "        p_inds = p_inds.squeeze(1)\n",
    "        n_inds = n_inds.squeeze(1)\n",
    "        return dist_ap, dist_an, p_inds, n_inds\n",
    "\n",
    "    return dist_ap, dist_an\n",
    "\n",
    "\n",
    "class TripletLoss(object):\n",
    "    \"\"\"Modified from Tong Xiao's open-reid (https://github.com/Cysu/open-reid).\n",
    "    Related Triplet Loss theory can be found in paper 'In Defense of the Triplet\n",
    "    Loss for Person Re-Identification'.\"\"\"\n",
    "\n",
    "    def __init__(self, margin=None):\n",
    "        self.margin = margin\n",
    "        if margin is not None:\n",
    "            self.ranking_loss = nn.MarginRankingLoss(margin=margin)\n",
    "        else:\n",
    "            self.ranking_loss = nn.SoftMarginLoss()\n",
    "\n",
    "    def __call__(self, global_feat, labels, normalize_feature=True):\n",
    "        if normalize_feature:\n",
    "            global_feat = normalize(global_feat, axis=-1)\n",
    "        dist_mat = euclidean_dist(global_feat, global_feat)\n",
    "        dist_ap, dist_an = hard_example_mining(\n",
    "            dist_mat, labels)\n",
    "        y = dist_an.new().resize_as_(dist_an).fill_(1)\n",
    "        if self.margin is not None:\n",
    "            loss = self.ranking_loss(dist_an, dist_ap, y)\n",
    "        else:\n",
    "            loss = self.ranking_loss(dist_an - dist_ap, y)\n",
    "        return loss\n",
    "\n",
    "class CrossEntropyLabelSmooth(nn.Module):\n",
    "    \"\"\"Cross entropy loss with label smoothing regularizer.\n",
    "    Reference:\n",
    "    Szegedy et al. Rethinking the Inception Architecture for Computer Vision. CVPR 2016.\n",
    "    Equation: y = (1 - epsilon) * y + epsilon / K.\n",
    "    Args:\n",
    "        num_classes (int): number of classes.\n",
    "        epsilon (float): weight.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes, epsilon=0.1, use_gpu=True):\n",
    "        super(CrossEntropyLabelSmooth, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.epsilon = epsilon\n",
    "        self.use_gpu = use_gpu\n",
    "        self.logsoftmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            inputs: prediction matrix (before softmax) with shape (batch_size, num_classes)\n",
    "            targets: ground truth labels with shape (num_classes)\n",
    "        \"\"\"\n",
    "        log_probs = self.logsoftmax(inputs)\n",
    "        targets = torch.zeros(log_probs.size()).scatter_(1, targets.unsqueeze(1).data.cpu(), 1)\n",
    "        if self.use_gpu: targets = targets.cuda()\n",
    "        targets = (1 - self.epsilon) * targets + self.epsilon / self.num_classes\n",
    "        loss = (- targets * log_probs).mean(0).sum()\n",
    "        return loss\n",
    "\n",
    "from typing import Tuple\n",
    "from torch import Tensor\n",
    "from bisect import bisect_right\n",
    "def convert_label_to_similarity(normed_feature: Tensor, label: Tensor) -> Tuple[Tensor, Tensor]:\n",
    "    similarity_matrix = normed_feature @ normed_feature.transpose(1, 0)\n",
    "    label_matrix = label.unsqueeze(1) == label.unsqueeze(0)\n",
    "\n",
    "    positive_matrix = label_matrix.triu(diagonal=1)\n",
    "    negative_matrix = label_matrix.logical_not().triu(diagonal=1)\n",
    "\n",
    "    similarity_matrix = similarity_matrix.view(-1)\n",
    "    positive_matrix = positive_matrix.view(-1)\n",
    "    negative_matrix = negative_matrix.view(-1)\n",
    "    return similarity_matrix[positive_matrix], similarity_matrix[negative_matrix]\n",
    "\n",
    "class CircleLoss(nn.Module):\n",
    "    def __init__(self, m: float, gamma: float) -> None:\n",
    "        super(CircleLoss, self).__init__()\n",
    "        self.m = m\n",
    "        self.gamma = gamma\n",
    "        self.soft_plus = nn.Softplus()\n",
    "\n",
    "\n",
    "    def forward(self,feat,label) -> Tensor:\n",
    "\n",
    "        feat=normalize(feat,axis=-1)\n",
    "        sp, sn = convert_label_to_similarity(feat, label)\n",
    "        ap = torch.clamp_min(- sp.detach() + 1 + self.m, min=0.)\n",
    "        an = torch.clamp_min(sn.detach() + self.m, min=0.)\n",
    "\n",
    "        delta_p = 1 - self.m\n",
    "        delta_n = self.m\n",
    "\n",
    "        logit_p = - ap * (sp - delta_p) * self.gamma\n",
    "        logit_n = an * (sn - delta_n) * self.gamma\n",
    "\n",
    "        loss = self.soft_plus(torch.logsumexp(logit_n, dim=0) + torch.logsumexp(logit_p, dim=0))\n",
    "\n",
    "        return loss\n",
    "class WarmupMultiStepLR(torch.optim.lr_scheduler._LRScheduler):\n",
    "    def __init__(\n",
    "            self,\n",
    "            optimizer,\n",
    "            milestones,\n",
    "            gamma=0.1,\n",
    "            warmup_factor=1.0 / 3,\n",
    "            warmup_iters=500,\n",
    "            warmup_method=\"linear\",\n",
    "            last_epoch=-1,\n",
    "    ):\n",
    "        if not list(milestones) == sorted(milestones):\n",
    "            raise ValueError(\n",
    "                \"Milestones should be a list of\" \" increasing integers. Got {}\",\n",
    "                milestones,\n",
    "            )\n",
    "\n",
    "        if warmup_method not in (\"constant\", \"linear\"):\n",
    "            raise ValueError(\n",
    "                \"Only 'constant' or 'linear' warmup_method accepted\"\n",
    "                \"got {}\".format(warmup_method)\n",
    "            )\n",
    "        self.milestones = milestones\n",
    "        self.gamma = gamma\n",
    "        self.warmup_factor = warmup_factor\n",
    "        self.warmup_iters = warmup_iters\n",
    "        self.warmup_method = warmup_method\n",
    "        super(WarmupMultiStepLR, self).__init__(optimizer, last_epoch)\n",
    "\n",
    "    def get_lr(self):\n",
    "        warmup_factor = 1\n",
    "        if self.last_epoch < self.warmup_iters:\n",
    "            if self.warmup_method == \"constant\":\n",
    "                warmup_factor = self.warmup_factor\n",
    "            elif self.warmup_method == \"linear\":\n",
    "                alpha = self.last_epoch / self.warmup_iters\n",
    "                warmup_factor = self.warmup_factor * (1 - alpha) + alpha\n",
    "        return [\n",
    "            base_lr\n",
    "            * warmup_factor\n",
    "            * self.gamma ** bisect_right(self.milestones, self.last_epoch)\n",
    "            for base_lr in self.base_lrs\n",
    "        ]\n",
    "class Flat(torch.optim.lr_scheduler._LRScheduler):\n",
    "    def __init__(self, optimizer, cfg,start_lr,anneal_start=0.6,t=4, last_epoch=-1):\n",
    "        self.epochs = cfg['epochs']\n",
    "        self.start = anneal_start\n",
    "        self.t=t\n",
    "        self.start_lr=start_lr\n",
    "\n",
    "\n",
    "        super(Flat, self).__init__(optimizer, last_epoch)\n",
    "\n",
    "    def get_lr(self):\n",
    "        if self.last_epoch < int(self.epochs * self.start):\n",
    "            return [base_lr for base_lr in self.base_lrs]\n",
    "        else:\n",
    "            return [\n",
    "                1e-5+(self.start_lr-1e-5)*(1+math.cos(math.pi*(self.last_epoch-int(self.epochs*self.start))/t))/2\n",
    "                for base_lr in self.base_lrs\n",
    "            ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneralizedMeanPooling(nn.Module):\n",
    "    r\"\"\"Applies a 2D power-average adaptive pooling over an input signal composed of several input planes.\n",
    "    The function computed is: :math:`f(X) = pow(sum(pow(X, p)), 1/p)`\n",
    "        - At p = infinity, one gets Max Pooling\n",
    "        - At p = 1, one gets Average Pooling\n",
    "    The output is of size H x W, for any input size.\n",
    "    The number of output features is equal to the number of input planes.\n",
    "    Args:\n",
    "        output_size: the target output size of the image of the form H x W.\n",
    "                     Can be a tuple (H, W) or a single H for a square image H x H\n",
    "                     H and W can be either a ``int``, or ``None`` which means the size will\n",
    "                     be the same as that of the input.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, norm, output_size=1, eps=1e-6):\n",
    "        super(GeneralizedMeanPooling, self).__init__()\n",
    "        assert norm > 0\n",
    "        self.p = float(norm)\n",
    "        self.output_size = output_size\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.clamp(min=self.eps).pow(self.p)\n",
    "        return torch.nn.functional.adaptive_avg_pool2d(x, self.output_size).pow(1. / self.p)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + '(' \\\n",
    "               + str(self.p) + ', ' \\\n",
    "               + 'output_size=' + str(self.output_size) + ')'\n",
    "\n",
    "\n",
    "class GeneralizedMeanPoolingP(GeneralizedMeanPooling):\n",
    "    \"\"\" Same, but norm is trainable\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, norm=3, output_size=1, eps=1e-6):\n",
    "        super(GeneralizedMeanPoolingP, self).__init__(norm, output_size, eps)\n",
    "        self.p = nn.Parameter(torch.ones(1) * norm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import resnet50\n",
    "from geffnet import efficientnet_b0, efficientnet_b1\n",
    "def build_backbone(model_name):\n",
    "    if model_name=='resnet50':\n",
    "        model = resnet50(True)\n",
    "        stride = 1\n",
    "        model.layer4[0].downsample[0].stride = stride\n",
    "        model.layer4[0].conv2.stride = stride\n",
    "        base = nn.Sequential(\n",
    "            model.conv1,\n",
    "            model.bn1,\n",
    "            model.maxpool,\n",
    "            model.layer1,\n",
    "            model.layer2,\n",
    "            model.layer3,\n",
    "            model.layer4\n",
    "        )\n",
    "        feat=2048\n",
    "    elif model_name == 'eff_b0':\n",
    "        backbone = efficientnet_b0(True)\n",
    "        backbone = nn.Sequential(\n",
    "            backbone.conv_stem,\n",
    "            backbone.bn1,\n",
    "            backbone.act1,\n",
    "            backbone.blocks,\n",
    "            backbone.conv_head,\n",
    "            backbone.bn2,\n",
    "            backbone.act2,\n",
    "            nn.Conv2d(1280, 2048, 1)\n",
    "        )\n",
    "        base = backbone\n",
    "        feat = 2048\n",
    "    elif model_name == 'eff_b1':\n",
    "        backbone = efficientnet_b1(True)\n",
    "        backbone = nn.Sequential(\n",
    "            backbone.conv_stem,\n",
    "            backbone.bn1,\n",
    "            backbone.act1,\n",
    "            backbone.blocks,\n",
    "            backbone.conv_head,\n",
    "            backbone.bn2,\n",
    "            backbone.act2,\n",
    "            nn.Conv2d(1280, 2048, 1)\n",
    "        )\n",
    "        base = backbone\n",
    "        feat = 2048\n",
    "    return base,feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import math\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "class Mask_Branch(nn.Module):\n",
    "    def __init__(self,cfg,num_class):\n",
    "        super(Mask_Branch, self).__init__()\n",
    "        self.cls_criterion=CrossEntropyLabelSmooth(num_class)\n",
    "        self.cls_criterion.to('cuda:0')\n",
    "        self.circle_criterion=CircleLoss(m=0.25,gamma=80)\n",
    "        self.circle_criterion.to('cuda:0')\n",
    "        self.N=cfg['N']\n",
    "        self.M=cfg['M']\n",
    "        self.lambdak=nn.Parameter(torch.from_numpy(np.array([0.05]*self.N)))\n",
    "        self.a0=-1*math.pi\n",
    "        self.b0=1*math.pi\n",
    "        self.a1=-1*math.pi/4\n",
    "        self.b1=-3*math.pi/4\n",
    "        self.a2=3*math.pi/4\n",
    "        self.b2=math.pi/4\n",
    "        self.a3 = 5*math.pi/8\n",
    "        self.b3 = 3*math.pi/8\n",
    "        self.lambdad = nn.Parameter(torch.from_numpy(np.array([0.05]*self.M)))\n",
    "        self.R_min=0 \n",
    "        self.R_max=2\n",
    "        self.backbone_list=nn.ModuleList()\n",
    "        for i in range(4):\n",
    "            backbone,feat_dim=build_backbone(cfg['backbone'])\n",
    "            self.backbone_list.append(backbone)\n",
    "\n",
    "        self.pool_list=nn.ModuleList()\n",
    "        for i in range(4):\n",
    "            for j in range(4):\n",
    "                pool=GeneralizedMeanPoolingP()\n",
    "                self.pool_list.append(pool)\n",
    "\n",
    "        self.ASE_FC_list=nn.ModuleList()\n",
    "        self.ASE_CONV_list=nn.ModuleList()\n",
    "        for i in range(4):\n",
    "            for j in range(4):\n",
    "                fc_block=nn.Sequential(\n",
    "                    nn.Linear(2048,1024),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(1024,2048),\n",
    "                )\n",
    "                conv_b=nn.Sequential(\n",
    "                    nn.Conv2d(2048,1024,kernel_size=1),\n",
    "                    nn.BatchNorm2d(1024),\n",
    "                    nn.ReLU()\n",
    "                )\n",
    "                self.ASE_FC_list.append(fc_block)\n",
    "                self.ASE_CONV_list.append(conv_b)\n",
    "\n",
    "        self.fc_list=nn.ModuleList()\n",
    "        for i in range(4):\n",
    "            for j in range(4):\n",
    "                fc=nn.Linear(1024,num_class)\n",
    "                self.fc_list.append(fc)\n",
    "\n",
    "    def ASE(self,x,index):\n",
    "        feat=self.ASE_FC_list[index](x.view(x.shape[0],-1))\n",
    "        x=x+torch.mul(x,F.sigmoid(feat).view(x.shape[0],-1,1,1))\n",
    "        x=self.ASE_CONV_list[index](x)\n",
    "        return x\n",
    "\n",
    "\n",
    "    def forward(self,x,label):\n",
    "        theta_list0 = []\n",
    "        theta_list1 = []\n",
    "        theta_list2 = []\n",
    "        theta_list3 = []\n",
    "        for i in range(self.N):\n",
    "            zi = self.lambdak[:i].sum() / self.lambdak.sum()\n",
    "            fi = (self.b0 - self.a0) * zi + self.a0\n",
    "            theta_list0.append(fi)\n",
    "        for i in range(self.N):\n",
    "            zi = self.lambdak[:i].sum() / self.lambdak.sum()\n",
    "            fi = (self.b1 - self.a1) * zi + self.a1\n",
    "            theta_list1.append(fi)\n",
    "        for i in range(self.N):\n",
    "            zi = self.lambdak[:i].sum() / self.lambdak.sum()\n",
    "            fi = (self.b2 - self.a2) * zi + self.a2\n",
    "            theta_list2.append(fi)\n",
    "        for i in range(self.N):\n",
    "            zi = self.lambdak[:i].sum() / self.lambdak.sum()\n",
    "            fi = (self.b3 - self.a3) * zi + self.a3\n",
    "            theta_list3.append(fi)\n",
    "        r_list = []\n",
    "        for i in range(self.M):\n",
    "            zi = self.lambdad[:i].sum()/self.lambdad.sum()\n",
    "            fi = (self.R_max - self.R_min) * zi + self.R_min\n",
    "            r_list.append(fi)\n",
    "        theta_lists=torch.tensor([theta_list0, theta_list1, theta_list2, theta_list3])\n",
    "        r_list = torch.tensor(r_list)\n",
    "        self.grid = torch.zeros((4, 1, self.N, self.M, 2))\n",
    "        for k in range(4):\n",
    "            for i in range(self.N):\n",
    "                for j in range(self.M):\n",
    "                    self.grid[k,0,i,j,0]=r_list[j]*torch.sin(theta_lists[k,i])\n",
    "                    self.grid[k,0,i,j,1]=r_list[j]*torch.cos(theta_lists[k,i])\n",
    "        self.grid = self.grid.cuda()\n",
    "\n",
    "        grid_features=[]\n",
    "        for i in range(4):\n",
    "            x=x.float()\n",
    "            grid=self.grid[i]\n",
    "            grid=grid.repeat(x.shape[0],1,1,1)\n",
    "            grid_feature=F.grid_sample(x,grid)\n",
    "            grid_features.append(grid_feature)\n",
    "        loss=0\n",
    "        logit_list=[]\n",
    "        final_feature_list=[]\n",
    "        #grid_features 4 x[0] 3 n m\n",
    "        for i in range(4):\n",
    "            features=self.backbone_list[i](grid_features[i])\n",
    "            stride=int(features.shape[2]/4)\n",
    "            for j in range(4):\n",
    "                feature=features[:,:,j*stride:(j+1)*stride,:]\n",
    "                feature=self.pool_list[i*4+j](feature)\n",
    "                feature=self.ASE(feature,i*4+j)\n",
    "                feature=feature.view(feature.shape[0],-1)\n",
    "                logit=self.fc_list[i*4+j](feature)\n",
    "                logit_list.append(logit)\n",
    "                final_feature_list.append(feature)\n",
    "        feat=torch.cat(final_feature_list,dim=1)\n",
    "        if self.training:        \n",
    "          for logit in logit_list:\n",
    "              loss=loss+self.cls_criterion(logit,label)\n",
    "\n",
    "          \n",
    "          loss=loss+self.circle_criterion(feat,label)\n",
    "        else:\n",
    "          loss=0\n",
    "        return feat,loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MARKET_EVAL_FUNC(dismat,q_pids,g_pids,q_camids,g_camids,max_rank=50):\n",
    "    '''\n",
    "    :param dismat: 每个query与其对应查询结果的距离\n",
    "    :param q_pids: 每个query行人的id\n",
    "    :param g_pids: 每个查询结果的行人的id\n",
    "    :param q_camids: 摄像头id，对于同一个query，如果查询结果与query都来自一个摄像头，则丢弃\n",
    "    :param g_camids: 同上\n",
    "    :param max_rank:\n",
    "    :return:\n",
    "    '''\n",
    "    num_q,num_g=dismat.shape\n",
    "    if num_g<max_rank:\n",
    "        max_rank=num_g\n",
    "\n",
    "    ##根据相似度进行排序\n",
    "    indices=np.argsort(dismat,axis=1)\n",
    "    \n",
    "    \n",
    "    ##在排序结果找到同id图片，用于计算cmc\n",
    "    matchs=(g_pids[indices]==q_pids[:,np.newaxis]).astype(np.int32)\n",
    "    \n",
    "\n",
    "\n",
    "    all_cmc=[]\n",
    "    all_AP=[]\n",
    "    num_valid_q=0\n",
    "\n",
    "    for q_idx in range(num_q):\n",
    "        q_pid=q_pids[q_idx]\n",
    "        q_camid=q_camids[q_idx]\n",
    "\n",
    "        '''当前query对应查询结果的排序'''\n",
    "        order=indices[q_idx]\n",
    "\n",
    "        '''若查询结果与query同摄像机且同id，删除'''\n",
    "        remove=(g_pids[order]==q_pid)&(g_camids[order]==q_camid)\n",
    "        keep=np.invert(remove)\n",
    "\n",
    "        '''计算CMC'''\n",
    "\n",
    "        '''首先先筛选'''\n",
    "        orig_cmc=matchs[q_idx][keep]\n",
    "\n",
    "        '''全是false'''\n",
    "        if not np.any(orig_cmc):\n",
    "            continue\n",
    "\n",
    "        \n",
    "\n",
    "        '''累加和，因为orig_cmc里面是true，false，true为1，得到cmc列表'''\n",
    "        cmc=orig_cmc.cumsum()\n",
    "        cmc[cmc>1]=1\n",
    "        all_cmc.append(cmc[:max_rank])\n",
    "        num_valid_q+=1 \n",
    "\n",
    "        '''计算map'''\n",
    "\n",
    "        '''预测正确的数量'''\n",
    "        num_rel=orig_cmc.sum()\n",
    "\n",
    "        '''累加和，用于计算'''\n",
    "        tmp_cmc=orig_cmc.cumsum()\n",
    "\n",
    "        '''计算准确率，前k个查询结果的准确率'''\n",
    "        tmp_cmc=[x/(i+1) for i,x in enumerate(tmp_cmc)]\n",
    "\n",
    "        '''计算召回率，召回率每次变化都是当查询结果id等于query时，故乘orig_cmc'''\n",
    "        '''最终的结果类似 0,0,1,0,0,2,0,0,3这种，每个有值的都是查询正确的'''\n",
    "        '''然后后面除以查询结果中同id数量，也就是上面num_rel就是找召回率'''\n",
    "        '''但是这里其实已经提前乘上准确率了，最终结果类似于 0,0,1/3,0,0,2/6,0,0,3/9'''\n",
    "        tmp_cmc=np.asarray(tmp_cmc)*orig_cmc\n",
    "\n",
    "        '''以上为例，除以同id数量后，假设为五个'''\n",
    "        '''0，0，1/3 /5，0，0，2/6 /5，0，0，3/9 /5'''\n",
    "        '''等价于 0,0, 1/3(准确率)*1/5(召回率) ....'''\n",
    "        \n",
    "        if num_rel==0:\n",
    "          AP=0\n",
    "          print('AP=0')\n",
    "        else:  \n",
    "          AP = tmp_cmc.sum() / num_rel\n",
    "          \n",
    "        all_AP.append(AP)\n",
    "\n",
    "    all_cmc = np.asarray(all_cmc).astype(np.float32)\n",
    "    all_cmc = all_cmc.sum(0) / num_valid_q\n",
    "    if len(all_AP)==0:\n",
    "      \n",
    "      return 0,0\n",
    "    mAP = np.mean(all_AP)\n",
    "\n",
    "    return all_cmc,mAP\n",
    "\n",
    "\n",
    "class MARKET_MAP():\n",
    "    def __init__(self,num_query,max_rank=50,feat_norm=True,one_day=True):\n",
    "        '''\n",
    "\n",
    "        :param num_query: 查询数量\n",
    "        :param max_rank: 每个query限定的最大查询结果数量\n",
    "        :param feat_norm:是否对特征进行l2norm\n",
    "        '''\n",
    "        self.num_query=num_query\n",
    "        self.max_rank=max_rank\n",
    "        self.feat_norm=feat_norm\n",
    "        self.one_day=one_day\n",
    "        \n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        '''\n",
    "        feats:模型返回的特征\n",
    "        pids: 图片中行人的id\n",
    "        camids:图片摄像头的id\n",
    "        :return:\n",
    "        '''\n",
    "        self.feats=[]\n",
    "        self.pids=[]\n",
    "        self.camids=[]\n",
    "        self.dates=[]\n",
    "\n",
    "    def update(self,output):\n",
    "        feat,pid,camid,date=output\n",
    "    \n",
    "        self.feats.append(feat)\n",
    "        self.pids.append(pid)\n",
    "        self.camids.append(camid)\n",
    "        self.dates.append(date)\n",
    "\n",
    "    def compute(self):\n",
    "        feats=torch.stack(self.feats,dim=0)\n",
    "          \n",
    "        if self.feat_norm:\n",
    "            feats=torch.nn.functional.normalize(feats,p=2)\n",
    "\n",
    "        \n",
    "        '''用于查询的图片的特征及其id和摄像头id'''\n",
    "        \n",
    "        qf=feats[:self.num_query]\n",
    "        q_pids=np.asarray(self.pids[:self.num_query])\n",
    "        q_camids = np.asarray(self.camids[:self.num_query])\n",
    "        q_dates=np.asarray(self.dates[:self.num_query])  \n",
    "\n",
    "        '''每个query查询结果的特征及其id和摄像头id'''\n",
    "        gf = feats[self.num_query:]\n",
    "        g_pids = np.asarray(self.pids[self.num_query:])\n",
    "        g_camids = np.asarray(self.camids[self.num_query:])\n",
    "        g_dates=np.asarray(self.dates[self.num_query:])\n",
    "       \n",
    "        if self.one_day:\n",
    "            \n",
    "            mean_map=0\n",
    "            date=0\n",
    "              \n",
    "            q_index=np.where(q_dates==date)\n",
    "            \n",
    "            date_qf=qf[q_index]\n",
    "            date_q_pids=q_pids[q_index]\n",
    "            date_q_camids=q_camids[q_index]\n",
    "\n",
    "            g_index=np.where(g_dates==date)\n",
    "            date_gf=gf[g_index]\n",
    "            date_g_pids=g_pids[g_index]\n",
    "            date_g_camids=g_camids[g_index]\n",
    "            m,n=date_qf.shape[0],date_gf.shape[0]\n",
    "            \n",
    "            if m==0:\n",
    "              return 0\n",
    "            distmat = torch.pow(date_qf, 2).sum(dim=1, keepdim=True).expand(m, n) + \\\n",
    "                  torch.pow(date_gf, 2).sum(dim=1, keepdim=True).expand(n, m).t()\n",
    "            \n",
    "            distmat.addmm_(1, -2, date_qf, date_gf.t())\n",
    "            distmat = distmat.cpu().numpy()\n",
    "            \n",
    "            cmc,mAP=MARKET_EVAL_FUNC(distmat,date_q_pids,date_g_pids,date_q_camids,date_g_camids)\n",
    "            mean_map+=mAP\n",
    "            return cmc,mean_map\n",
    "        else:\n",
    "           \n",
    "            mean_map=0\n",
    "            \n",
    "            date=0\n",
    "            q_index=np.where(q_dates==date)\n",
    "            \n",
    "            date_qf=qf[q_index]\n",
    "            date_q_pids=q_pids[q_index]\n",
    "            date_q_camids=q_camids[q_index]\n",
    "\n",
    "            g_index=np.where(g_dates!=date)\n",
    "            date_gf=gf[g_index]\n",
    "            date_g_pids=g_pids[g_index]\n",
    "            date_g_camids=g_camids[g_index]\n",
    "            m,n=date_qf.shape[0],date_gf.shape[0]\n",
    "  \n",
    "            if m==0 or n==0:\n",
    "               return 0\n",
    "            distmat = torch.pow(date_qf, 2).sum(dim=1, keepdim=True).expand(m, n) + \\\n",
    "                  torch.pow(date_gf, 2).sum(dim=1, keepdim=True).expand(n, m).t()\n",
    "            \n",
    "            distmat.addmm_(1, -2, date_qf, date_gf.t())\n",
    "            distmat = distmat.cpu().numpy()\n",
    "            \n",
    "            cmc,mAP=MARKET_EVAL_FUNC(distmat,date_q_pids,date_g_pids,date_q_camids,date_g_camids)\n",
    "            mean_map+=mAP\n",
    "            return cmc,mean_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Checkpoint:\n",
    "    def __init__(self, ckpt):\n",
    "        self.ckpt = ckpt\n",
    "        self.init = -1000\n",
    "\n",
    "    def __call__(self, metric, model):\n",
    "        if metric > self.init:\n",
    "            self.init = metric\n",
    "            torch.save({'model': model.state_dict()}, self.ckpt)\n",
    "\n",
    "class EMA():\n",
    "    def __init__(self, model, decay):\n",
    "        self.model = model\n",
    "        self.decay = decay\n",
    "        self.shadow = {}\n",
    "        self.backup = {}\n",
    "\n",
    "    def register(self):\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                self.shadow[name] = param.data.clone()\n",
    "\n",
    "    def update(self):\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                assert name in self.shadow\n",
    "                new_average = (1.0 - self.decay) * param.data + self.decay * self.shadow[name]\n",
    "                self.shadow[name] = new_average.clone()\n",
    "\n",
    "    def apply_shadow(self):\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                assert name in self.shadow\n",
    "                self.backup[name] = param.data\n",
    "                param.data = self.shadow[name]\n",
    "\n",
    "    def restore(self):\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                assert name in self.backup\n",
    "                param.data = self.backup[name]\n",
    "        self.backup = {}\n",
    "\n",
    "\n",
    "\n",
    "# NOTE(avik-pal): This code has been adapted from\n",
    "#                 https://github.com/heykeetae/Self-Attention-GAN/blob/master/spectral.py\n",
    "class SpectralNorm2d(nn.Module):\n",
    "    r\"\"\"2D Spectral Norm Module as described in `\"Spectral Normalization\n",
    "    for Generative Adversarial Networks by Miyato et. al.\" <https://arxiv.org/abs/1802.05957>`_\n",
    "    The spectral norm is computed using ``power iterations``.\n",
    "    Computation Steps:\n",
    "    .. math:: v_{t + 1} = \\frac{W^T W v_t}{||W^T W v_t||} = \\frac{(W^T W)^t v}{||(W^T W)^t v||}\n",
    "    .. math:: u_{t + 1} = W v_t\n",
    "    .. math:: v_{t + 1} = W^T u_{t + 1}\n",
    "    .. math:: Norm(W) = ||W v|| = u^T W v\n",
    "    .. math:: Output = \\frac{W}{Norm(W)} = \\frac{W}{u^T W v}\n",
    "    Args:\n",
    "        module (torch.nn.Module): The Module on which the Spectral Normalization needs to be\n",
    "            applied.\n",
    "        name (str, optional): The attribute of the ``module`` on which normalization needs to\n",
    "            be performed.\n",
    "        power_iterations (int, optional): Total number of iterations for the norm to converge.\n",
    "            ``1`` is usually enough given the weights vary quite gradually.\n",
    "    Example:\n",
    "        .. code:: python\n",
    "            >>> layer = SpectralNorm2d(Conv2d(3, 16, 1))\n",
    "            >>> x = torch.rand(1, 3, 10, 10)\n",
    "            >>> layer(x)\n",
    "    \"\"\"\n",
    "    def __init__(self, module, name=\"weight\", power_iterations=1):\n",
    "        super(SpectralNorm2d, self).__init__()\n",
    "        self.module = module\n",
    "        self.name = name\n",
    "        self.power_iterations = power_iterations\n",
    "        w = getattr(self.module, self.name)\n",
    "        height = w.data.shape[0]\n",
    "        width = w.view(height, -1).data.shape[1]\n",
    "        self.u = nn.Parameter(w.data.new(height).normal_(0, 1), requires_grad=False)\n",
    "        self.v = nn.Parameter(w.data.new(width).normal_(0, 1), requires_grad=False)\n",
    "        self.u.data = self._l2normalize(self.u.data)\n",
    "        self.v.data = self._l2normalize(self.v.data)\n",
    "        self.w_bar = nn.Parameter(w.data)\n",
    "        del self.module._parameters[self.name]\n",
    "\n",
    "    def _l2normalize(self, x, eps=1e-12):\n",
    "        r\"\"\"Function to calculate the ``L2 Normalized`` form of a Tensor\n",
    "        Args:\n",
    "            x (torch.Tensor): Tensor which needs to be normalized.\n",
    "            eps (float, optional): A small value needed to avoid infinite values.\n",
    "        Returns:\n",
    "            Normalized form of the tensor ``x``.\n",
    "        \"\"\"\n",
    "        return x / (torch.norm(x) + eps)\n",
    "\n",
    "    def forward(self, *args):\n",
    "        r\"\"\"Computes the output of the ``module`` and appies spectral normalization to the\n",
    "        ``name`` attribute of the ``module``.\n",
    "        Returns:\n",
    "            The output of the ``module``.\n",
    "        \"\"\"\n",
    "        height = self.w_bar.data.shape[0]\n",
    "        for _ in range(self.power_iterations):\n",
    "            self.v.data = self._l2normalize(\n",
    "                torch.mv(torch.t(self.w_bar.view(height, -1)), self.u)\n",
    "            )\n",
    "            self.u.data = self._l2normalize(\n",
    "                torch.mv(self.w_bar.view(height, -1), self.v)\n",
    "            )\n",
    "        sigma = self.u.dot(self.w_bar.view(height, -1).mv(self.v))\n",
    "        setattr(self.module, self.name, self.w_bar / sigma.expand_as(self.w_bar))\n",
    "        return self.module.forward(*args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import logging\n",
    "import albumentations as albu\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchtools.optim import RangerLars\n",
    "from torch.cuda.amp import autocast,GradScaler\n",
    "\n",
    "scaler=GradScaler()\n",
    "class Trainer:\n",
    "  def __init__(self,cfg,train_component):\n",
    "    self.train_loader=train_component['train_loader']\n",
    "    self.val_loader=train_component['val_loader']\n",
    "    self.reid_model=train_component['model']\n",
    "    self.optimizer=train_component['optimizer']\n",
    "    self.scheduler=train_component['scheduler']\n",
    "    self.logger=train_component['logger']\n",
    "    self.num_class=train_component['num_class']\n",
    "    self.num_query=train_component['num_query']\n",
    "    self.checkpoint=train_component['checkpoint']\n",
    "    \n",
    "    if cfg['ema']:\n",
    "      self.ema=train_component['ema']\n",
    "   \n",
    "    self.device=torch.device('cuda:0')\n",
    "    self.reid_model=self.reid_model.to(self.device)\n",
    "    self.cfg=cfg\n",
    "    \n",
    "    \n",
    "  def train_one_epoch(self):\n",
    "    face_id=0\n",
    "   \n",
    "    for images,sketchs,pids in self.train_loader:\n",
    "      loss=0\n",
    "      self.optimizer.zero_grad()\n",
    "     \n",
    "      \n",
    "      sketchs=sketchs.to(self.device)\n",
    "      pids=pids.to(self.device)\n",
    "      with autocast():\n",
    "        _,loss=self.reid_model(sketchs,pids)\n",
    "        \n",
    "      scaler.scale(loss).backward()\n",
    "      scaler.step(self.optimizer)\n",
    "      scaler.update() \n",
    "      if self.cfg['ema']:\n",
    "        self.ema.update()\n",
    "     \n",
    "  \n",
    "  \n",
    "  def train(self):\n",
    "        epochs=self.cfg['epochs']\n",
    "        \n",
    "        \n",
    "        with tqdm(total=epochs) as pbar:\n",
    "          for epoch in range(epochs):\n",
    "              self.reid_model.train()\n",
    "              self.train_one_epoch()\n",
    "              print(1)\n",
    "              if epoch%3==0:\n",
    "                  if self.cfg['ema']:\n",
    "                      self.ema.apply_shadow()\n",
    "                  map0,map1,cmc0,cmc1=self.eval()\n",
    "                  print('epoch:{}--map0:{}---map1:{}---cmc0:{}---cmc1:{}'.format(epoch,map0,map1,cmc0[0],cmc1[0]))\n",
    "                  self.logger.info('epoch:{}--map0:{}---map1:{}'.format(epoch,map0,map1))\n",
    "                  self.checkpoint((map0+map1)/2,self.reid_model)\n",
    "                  if self.cfg['ema']:\n",
    "                        self.ema.restore()    \n",
    "              pbar.update(1)\n",
    "     \n",
    "        \n",
    "  def eval(self):\n",
    "    self.reid_model.eval()\n",
    "    \n",
    "     \n",
    "    metric0=MARKET_MAP(self.num_query,one_day=True)\n",
    "    metric1=MARKET_MAP(self.num_query,one_day=False)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for imgs,sketchs,pids,cams,dates in self.val_loader:\n",
    "  \n",
    "          sketchs=sketchs.to(self.device)  \n",
    "          feat,_=self.reid_model(sketchs,pids)\n",
    "          \n",
    "          for out in zip(feat,pids,cams,dates):\n",
    "            \n",
    "\n",
    "            metric0.update(out)\n",
    "            metric1.update(out)\n",
    "            \n",
    "    cmc0,map0=metric0.compute()\n",
    "    cmc1,map1=metric1.compute()\n",
    "    return map0,map1,cmc0,cmc1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "def main(cfg):\n",
    "  dataset=PRCC(cfg['root'])\n",
    "  print(len(dataset.train))\n",
    "  train_loader,val_loader,num_query,num_classes=make_dataloader(dataset,cfg)\n",
    "  model=Mask_Branch(cfg,num_classes)\n",
    "  if cfg['optimizer']=='adam':\n",
    "    optimizer=torch.optim.Adam(model.parameters(),lr=cfg['lr'],weight_decay=cfg['weight_decay'])\n",
    "  elif cfg['optimizer']=='sgd':\n",
    "    optimizer=torch.optim.SGD(model.parameters(),lr=cfg['lr'],momentum=cfg['momentum'],weight_decay=cfg['weight_decay'])\n",
    "  elif cfg['optimizer']=='rangerlars':\n",
    "    optimizer=RangerLars(model.parameters(),lr=cfg['lr'])\n",
    "  \n",
    "  if cfg['scheduler']=='warmup':\n",
    "\n",
    "    scheduler = WarmupMultiStepLR(optimizer,cfg['steps'], cfg['gamma'],\n",
    "                  cfg['warmup_factors'],cfg['warmup_iters'], \n",
    "                  'linear')\n",
    "  elif cfg['scheduler']=='flat':\n",
    "    scheduler=Flat(optimizer,cfg,cfg['lr'])\n",
    "  elif cfg['scheduler']=='cos':\n",
    "    scheduler=torch.optim.lr_scheduler.CosineAnnealingLR(optimizer,5)\n",
    "  \n",
    "  ckpt=cfg['ckpt']\n",
    "  logpt=cfg['logpt']\n",
    "  logger=logging.getLogger()\n",
    "  fh = logging.FileHandler(\"spam.log\")\n",
    "  fh.setLevel(logging.DEBUG)\n",
    "  logger.addHandler(fh)\n",
    "  checkpoint=Checkpoint(ckpt)\n",
    "  train_component={}\n",
    "  train_component['train_loader']=train_loader\n",
    "  train_component['val_loader']=val_loader\n",
    "  train_component['model']=model\n",
    "  train_component['optimizer']=optimizer\n",
    "  train_component['scheduler']=scheduler\n",
    "  train_component['logger']=logger\n",
    "  train_component['num_class']=num_classes\n",
    "  train_component['num_query']=num_query\n",
    "  train_component['checkpoint']=checkpoint\n",
    "  trainer=Trainer(cfg,train_component)\n",
    "  trainer.train()\n",
    "  return trainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "torch.backends.cudnn.benchmark=True\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)       \n",
    "cfg={\n",
    "\n",
    " \n",
    "    'num_workers':8,\n",
    "    'train_bs':64,\n",
    "    'test_bs':64,\n",
    "    'train_K_instances':16,\n",
    "     \n",
    "    'epochs':20,\n",
    "    'ckpt':'reid_test1.pt',\n",
    "    'logpt':'test1.log',\n",
    "   \n",
    "    'momentum':0.9,\n",
    "    'weight_decay':5e-4,\n",
    "     \n",
    "    'margin':0.3,\n",
    "    'steps':[35,55],\n",
    "    'warmup_iters':10,\n",
    "    'warmup_factors':1/3,\n",
    "    'gamma':0.1,\n",
    "    'backbone':'eff_b0',\n",
    "    'optimizer':'rangerlars',\n",
    "    'scheduler':'cos',\n",
    "   \n",
    "    'ema':False,\n",
    "    'N':128,\n",
    "    'M':128,\n",
    "    'lr':1e-3,\n",
    "    'root':'/kaggle/working/datasets/prcc_proprecess'\n",
    "}   \n",
    "\n",
    "trainer=main(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test\n",
    "map0,map1,cmc0,cmc1 = trainer.eval()\n",
    "print(\"Between camera A and B(without clothes changing):\")\n",
    "print(\"Rank1 Acc:%.2f\\nRank10 Acc:%.2f\\nRank20 Acc:%.2f\"%(cmc0[0],cmc0[9],cmc0[19]))\n",
    "print(\"Between camera A and C(with clothes changing):\")\n",
    "print(\"Rank1 Acc:%.2f\\nRank10 Acc:%.2f\\nRank20 Acc:%.2f\"%(cmc1[0],cmc1[9],cmc1[19]))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ae51ee3d492f24e83e77a52eb34bf16365894f8747390aa8e17995579dedf394"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
